<!DOCTYPE html>
<html lang="en">
<head>
        <title>MinHashing all the things: searching for MAGs in the SRA</title>
        <link rel="shortcut icon" href="/images/favicon.ico">
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        <link rel="stylesheet" href="https://blog.luizirber.org/theme/css/pygments.css" type="text/css" />
        <link rel="stylesheet" href="https://blog.luizirber.org/theme/css/main.css" type="text/css" />
        <link href="https://blog.luizirber.org/atom.xml" type="application/atom+xml" rel="alternate" title="Gabbleblotchits ATOM Feed" />
        <link href="https://blog.luizirber.org/feed.xml" type="application/rss+xml" rel="alternate" title="Gabbleblotchits RSS Feed" />
</head>

<body>

        <header>
          <h1>Gabbleblotchits</h1>

          <a href="mailto:contact@luizirber.org">
            <img class='email'
                 src='https://blog.luizirber.org/theme/icons/email.svg'
                 alt='email me'
                 title='email me'/>
          </a>
          <a href="https://social.lasanha.org/@luizirber">
            <img class='mastodon'
                 src='https://blog.luizirber.org/theme/icons/mastodon.svg'
                 alt='Mastodon icon'
                 title='Follow me on Mastodon'/>
          </a>
          <a href="https://github.com/luizirber">
            <img class='github'
                 src='https://blog.luizirber.org/theme/icons/github.svg'
                 alt='github projects'
                 title='github profile'/>
          </a>
          <a href="https://luizirber.newsblur.com">
            <img class='newsblur'
                 src='https://blog.luizirber.org/theme/icons/newsblur.svg'
                 alt='my shared stories on newsblur'
                 title='my shared stories on newsblur'/>
          </a>
          <a href="https://blog.luizirber.org/feed.xml" rel="alternative" type="application/rss+xml">
            <img class='rss'
                 src='https://blog.luizirber.org/theme/icons/feed.svg'
                 alt='RSS feed icon'
                 title='Subscribe to my RSS feed'/>
          </a>

<strong>Vogon Poetry, Computers and (some) biology</strong>        </header>


<nav>
    <a href="/">Home</a>
</nav>

<main>
    <h1>MinHashing all the things: searching for MAGs in the SRA</h1>

    <time class="single" datetime="2020-07-22T12:00:00-03:00">22 Jul 2020</time>

    <p>(or: Top-down and bottom-up approaches for working around sourmash limitations)</p>
<p>In the last month I updated <a href="https://wort.oxli.org">wort</a>,
the system I developed for computing sourmash signature for public genomic databases,
and started calculating signatures
for the <a href="https://www.ncbi.nlm.nih.gov/sra/?term=%22METAGENOMIC%22%5Bsource%5D+NOT+amplicon%5BAll+Fields%5D)">metagenomes</a> in the <a href="https://www.ncbi.nlm.nih.gov/sra/">Sequence Read Archive</a>.
This is a more challenging subset than the <a href="https://blog.luizirber.org/2016/12/28/soursigs-arch-1/">microbial datasets</a> I was doing previously, 
since there are around 534k datasets from metagenomic sources in the SRA,
totalling 447 TB of data.
Another problem is the size of the datasets,
ranging from a couple of MB to 170 GB.
Turns out that the workers I have in <code>wort</code> are very good for small-ish datasets,
but I still need to figure out how to pull large datasets faster from the SRA,
because the large ones take forever to process...</p>
<p>The good news is that I managed to calculate signatures for almost 402k of them
<sup id=sf-mag-search-1-back><a href=#sf-mag-search-1 class=simple-footnote title="pulling about a 100 TB in 3 days, which was pretty fun to see because I ended up DDoS myself because I couldn't download the generated sigs fast enough from the S3 bucket where they are temporarily stored =P">1</a></sup>,
which already let us work on some pretty exciting problems =]</p>
<h2>Looking for MAGs in the SRA</h2>
<p>Metagenome-assembled genomes are essential for studying organisms that are hard to isolate and culture in lab,
especially for environmental metagenomes.
<a href="https://www.nature.com/articles/sdata2017203">Tully et al</a> published 2,631 draft MAGs from 234 samples collected during the Tara Oceans expedition,
and I wanted to check if they can also be found in other metagenomes besides the Tara Oceans ones.
The idea is to extract the reads from these other matches and evaluate how the MAG can be improved,
or at least evaluate what is missing in them.
I choose to use environmental samples under the assumption they are easier to deposit on the SRA and have public access,
but there are many human gut microbiomes in the SRA and this MAG search would work just fine with those too.</p>
<p>Moreover,
I want to search for containment,
and not similarity.
The distinction is subtle,
but similarity takes into account both datasets sizes
(well, the size of the union of all elements in both datasets),
while containment only considers the size of the query.
This is relevant because the similarity of a MAG and a metagenome is going to be very small (and is symmetrical),
but the containment of the MAG in the metagenome might be large
(and is asymmetrical, since the containment of the metagenome in the MAG is likely very small because the metagenome is so much larger than the MAG).</p>
<h2>The computational challenge: indexing and searching</h2>
<p>sourmash signatures are a small fraction of the original size of the datasets,
but when you have hundreds of thousands of them the collection ends up being pretty large too.
More precisely, 825 GB large.
That is way bigger than any index I ever built for sourmash,
and it would also have pretty distinct characteristics than what we usually do: 
we tend to index genomes and run <code>search</code> (to find similar genomes) or <code>gather</code>
(to decompose metagenomes into their constituent genomes),
but for this MAG search I want to find which metagenomes have my MAG query above a certain containment threshold.
Sort of a <code>sourmash search --containment</code>,
but over thousands of metagenome signatures.
The main benefit of an SBT index in this context is to avoid checking all signatures because we can prune the search early,
but currently SBT indices need to be totally loaded in memory during <code>sourmash index</code>.
I will have to do this in the medium term,
but I want a solution NOW! =]</p>
<p><a href="https://github.com/dib-lab/sourmash/releases/tag/v3.4.0">sourmash 3.4.0</a> introduced <code>--from-file</code> in many commands,
and since I can't build an index I decided to use it to load signatures for the metagenomes.
But... <code>sourmash search</code> tries to load all signatures in memory,
and while I might be able to find a cluster machine with hundreds of GBs of RAM available, 
that's not very practical.</p>
<p>So, what to do?</p>
<h2>The top-down solution: a snakemake workflow</h2>
<p>I don't want to modify sourmash now,
so why not make a workflow and use snakemake to run one <code>sourmash search --containment</code> for each metagenome?
That means 402k tasks,
but at least I can use <a href="https://snakemake.readthedocs.io/en/stable/executing/cli.html#dealing-with-very-large-workflows">batches</a> and <a href="https://slurm.schedmd.com/job_array.html">SLURM job arrays</a> to submit reasonably-sized jobs to our HPC queue.
After running all batches I summarized results for each task,
and it worked well for a proof of concept.</p>
<p>But... it was still pretty resource intensive:
each task was running one query MAG against one metagenome,
and so each task needed to do all the overhead of starting the Python interpreter and parsing the query signature,
which is exactly the same for all tasks.
Extending it to support multiple queries to the same metagenome would involve duplicating tasks,
and 402k metagenomes times 2,631 MAGs is...
a very large number of jobs.</p>
<p>I also wanted to avoid clogging the job queues,
which is not very nice to the other researchers using the cluster.
This limited how many batches I could run in parallel...</p>
<h2>The bottom-up solution: Rust to the rescue!</h2>
<p>Thinking a bit more about the problem,
here is another solution:
what if we load all the MAGs in memory
(as they will be queried frequently and are not that large),
and then for each metagenome signature load it,
perform all MAG queries,
and then unload the metagenome signature from memory?
This way we can control memory consumption
(it's going to be proportional to all the MAG sizes plus the size of the largest metagenome)
and can also efficiently parallelize the code because each task/metagenome is independent
and the MAG signatures can be shared freely (since they are read-only).</p>
<p>This could be done with the sourmash Python API plus <code>multiprocessing</code> or some
other parallelization approach (maybe dask?),
but turns out that everything we need comes from the Rust API.
Why not enjoy a bit of the <a href="https://doc.rust-lang.org/stable/book/ch16-00-concurrency.html">fearless concurrency</a> that is one of the major Rust goals?</p>
<p><a href="https://github.com/luizirber/phd/blob/aa1ed9eb33ba71fdf9b3f2c92931701be6df00cd/experiments/wort/sra_search/src/main.rs">The whole code</a> ended up being 176 lines long,
including command line parsing using <a href="https://docs.rs/structopt/latest/structopt/">strucopt</a> and parallelizing the search using <a href="https://docs.rs/rayon/latest/rayon/">rayon</a>
and a <a href="https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html">multiple-producer, single-consumer channel</a> to write results to an output
(either the terminal or a file).
This version took 11 hours to run,
using less than 5GB of RAM and 32 processors,
to search 2k MAGs against 402k metagenomes.
And, bonus! It can also be parallelized again if you have multiple machines,
so it potentially takes a bit more than an hour to run if you can allocate 10 batch jobs,
with each batch 1/10 of the metagenome signatures.</p>
<h2>So, is bottom-up always the better choice?</h2>
<p>I would like to answer "Yes!",
but bioinformatics software tends to be organized as command line interfaces,
not as libraries.
Libraries also tend to have even less documentation than CLIs,
and this particular case is not a fair comparison because...
Well, I wrote most of the library,
and the Rust API is not that well documented for general use.</p>
<p>But I'm pretty happy with how the sourmash CLI is viable both for the top-down approach
(and whatever workflow software you want to use) as well as how the Rust core worked for the bottom-up approach.
I think the most important is having the option to choose which way to go,
especially because now I can use the bottom-up approach to make the sourmash CLI
and Python API better.
The top-down approach is also way more accessible in general,
because you can pick your favorite workflow software and use all the tricks you're comfortable with.</p>
<h2>But, what about the results?!?!?!</h2>
<p>Next time. But I did find MAGs with over 90% containment in very different locations,
which is pretty exciting!</p>
<p>I also need to find a better way of distributing all these signature,
because storing 4 TB of data in S3 is somewhat cheap,
but transferring data is very expensive.
All signatures are also available on IPFS,
but I need more people to host them and share.
Get in contact if you're interested in helping =]</p>
<p>And while I'm asking for help,
any tips on pulling data faster from the SRA are greatly appreciated!</p>
<h2>Comments?</h2>
<ul>
<li><a href="https://twitter.com/luizirber/status/1285782732790849537">Thread on Twitter</a></li>
</ul><section class=footnotes><hr><h2>Footnotes</h2><ol><li id=sf-mag-search-1><p>pulling about a 100 TB in 3 days, which was pretty fun to see because I
ended up DDoS myself because I couldn't download the generated sigs fast enough
from the S3 bucket where they are temporarily stored =P <a href=#sf-mag-search-1-back class=simple-footnote-back>â†©</a></p></li></ol></section>

    <div class="taglist"><p>Tags:
    <a href="https://blog.luizirber.org/tag/bioinformatics.html">bioinformatics</a>
    <a href="https://blog.luizirber.org/tag/rust.html">rust</a>
</p>
</div>
</main>

        <hr />
        <footer class="footer-menu">
            <a href="#top">Back to the top</a>
            <hr />
        </footer>

</body>
</html>